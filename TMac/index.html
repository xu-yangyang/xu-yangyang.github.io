<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="icon" href="../images/RPI.png" type="image/png" />
<title>TMac: Tensor completion by parallel Matrix factorization</title>
<!-- Yangyang: latest Google Analytics script -->
<script type="text/javascript">
  var _gaq = [];
  _gaq.push(['_setAccount', 'UA-61771387-1']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body>
<div class="wrapper"> <!-- Yangyang: "wrapper" class for new style -->
<div id="layout-content">
<div id="toptitle">
<h1>TMac: Tensor completion by parallel Matrix factorization</h1>
<div id="subtitle"><i><a href="https://xu-yangyang.github.io">Yangyang Xu</a>, Ruru Hao, <a href="http://www.math.ucla.edu/~wotaoyin/">Wotao Yin</a>, and Zhixun Su</i></div>
</div>
<h2>Background</h2>
<p>Higher-order low-rank tensors with missing values naturally arise in many applications including hyperspectral data recovery, video inpainting, seismic data reconstruction, and so on. These problems can be formulated as low-rank tensor completion (LRTC). Existing methods<img class="eq" src="eqs/7925648196134842906-130.png" alt="^{[1, 2]}" style="vertical-align: --2px" /> for LRTC employ matrix nuclear-norm minimization and use the singular value decomposition (SVD) in their algorithms, which become very slow or even not applicable for large-scale problems. To tackle this difficulty, we apply low-rank matrix factorization to each mode unfolding of the tensor in order to enforce low-rankness and update the matrix factors alternatively, which is computationally much cheaper than SVD.</p>
<h2>Our method</h2>
<p>We aim at recovering a low-rank tensor <img class="eq" src="eqs/5943952226732621904-130.png" alt="mathbf{mathcal{M}}inmathbf{R}^{I_1timesldotstimes I_N}" style="vertical-align: -1px" /> from partial observations <img class="eq" src="eqs/3734226386808237034-130.png" alt="mathbf{mathcal{B}}=mathcal{P}_Omega(mathbf{mathcal{M}})" style="vertical-align: -4px" />, where <img class="eq" src="eqs/3287431136534085787-130.png" alt="Omega" style="vertical-align: -0px" /> is the index set of observed entries, and <img class="eq" src="eqs/6631946050023744508-130.png" alt="mathcal{P}_Omega" style="vertical-align: -3px" /> keeps the entries in <img class="eq" src="eqs/3287431136534085787-130.png" alt="Omega" style="vertical-align: -0px" /> and zeros out others. We  apply low-rank matrix factorization to each mode unfolding of <img class="eq" src="eqs/3261975815629106195-130.png" alt="mathbf{mathcal{M}}" style="vertical-align: -1px" /> by finding matrices <img class="eq" src="eqs/5548413008889340189-130.png" alt="mathbf{X}_ninmathbf{R}^{I_ntimes r_n},mathbf{Y}_nin mathbf{R}^{r_ntimesPi_{jneq n}I_j}" style="vertical-align: -4px" /> such that <img class="eq" src="eqs/79582321679439465-130.png" alt="mathbf{M}_{(n)}approxmathbf{X}_nmathbf{Y}_n" style="vertical-align: -7px" /> for <img class="eq" src="eqs/3593854382826423542-130.png" alt="n=1,ldots,N" style="vertical-align: -4px" />, where <img class="eq" src="eqs/756880090336066486-130.png" alt="r_n" style="vertical-align: -3px" /> is the estimated rank, either fixed or adaptively updated. Introducing one common variable <img class="eq" src="eqs/3261990815757106462-130.png" alt="mathbf{mathcal{Z}}" style="vertical-align: -0px" /> to relate these matrix factorizations, we solve the following model</p>

<div class="eqwl"><img class="eqwl" src="eqs/3787947987456910049-130.png" alt="mathop{mathrm{minimize}}_{mathbf{X},mathbf{Y},mathbf{mathcal{Z}}}sum_{n=1}^Nfrac{alpha_n}{2}|mathbf{X}_nmathbf{Y}_n-mathbf{Z}_{(n)}|_F^2,mathrm{s.t.} mathcal{P}_Omega(mathbf{mathcal{Z}})=mathbf{mathcal{B}}, " />
<br /></div><p>where <img class="eq" src="eqs/4528113046417309844-130.png" alt="mathbf{X}=(mathbf{X}_1,ldots,mathbf{X}_N)" style="vertical-align: -4px" /> and <img class="eq" src="eqs/7050535594954597927-130.png" alt="mathbf{Y}=(mathbf{Y}_1,ldots,mathbf{Y}_N)" style="vertical-align: -4px" />. In the model, <img class="eq" src="eqs/2031553203773749405-130.png" alt="alpha_n" style="vertical-align: -3px" />, <img class="eq" src="eqs/3593854382826423542-130.png" alt="n=1,ldots,N" style="vertical-align: -4px" />, are weights and satisfy <img class="eq" src="eqs/3417472603050098989-130.png" alt="sum_nalpha_n=1" style="vertical-align: -6px" />. 
We use alternating least squares method to solve the model. </p>
<h2>Results</h2>
<p>Our model is non-convex jointly with respect to <img class="eq" src="eqs/6195215270188654494-130.png" alt="mathbf{X},mathbf{Y}" style="vertical-align: -4px" /> and <img class="eq" src="eqs/6379065522994502537-130.png" alt="mathcal{Z}" style="vertical-align: -0px" />. Although a global solution is not guaranteed, we demonstrate by numerical experiments that our algorithm can reliably recover a wide variety of low-rank tensors, such as the following phase transition plots. In the picture, each target tensor <img class="eq" src="eqs/456164137832015035-130.png" alt="mathcal{M}=mathcal{C}times_1mathbf{A}_1times_2mathbf{A}_2times_3mathbf{A}_3" style="vertical-align: -3px" />, where <img class="eq" src="eqs/1360531993777924342-130.png" alt="mathcal{C}inmathbf{R}^{rtimes rtimes r}" style="vertical-align: -1px" /> and <img class="eq" src="eqs/6277672749167673745-130.png" alt="mathbf{A}_ninmathbf{R}^{50times r}, forall n" style="vertical-align: -4px" /> have Gaussian random entries. (a) FaLRTC: the tensor completion method in <img class="eq" src="eqs/8209412804333245623-130.png" alt="[2]" style="vertical-align: -5px" />.
(b) MatComp: first reshape the tensor as a matrix and then use the matrix completion solver LMaFit in <img class="eq" src="eqs/8209412804332245752-130.png" alt="[3]" style="vertical-align: -5px" />. (c) TMac-fix: our method with <img class="eq" src="eqs/7197615725885034092-130.png" alt="alpha_n=frac{1}{3}" style="vertical-align: -7px" /> and <img class="eq" src="eqs/756880090336066486-130.png" alt="r_n" style="vertical-align: -3px" /> fixed to <img class="eq" src="eqs/6391779146996733402-130.png" alt="r, forall n" style="vertical-align: -4px" />. (d) TMac-inc: our method with <img class="eq" src="eqs/7197615725885034092-130.png" alt="alpha_n=frac{1}{3}" style="vertical-align: -7px" /> and using rank-increasing strategy starting from <img class="eq" src="eqs/959744891941625085-130.png" alt="r_n = mathrm{round}(0.75r),forall n" style="vertical-align: -4px" />. (e) TMac-dec: our method with<img class="eq" src="eqs/7197615725885034092-130.png" alt="alpha_n=frac{1}{3}" style="vertical-align: -7px" /> and using rank-decreasing strategy starting from <img class="eq" src="eqs/4798028595181042027-130.png" alt="r_n = mathrm{round}(1.25r),forall n" style="vertical-align: -4px" />.</p>
<p>The results show that our method performs much better than the other two methods.</p>
<table class="imgtable" align="center"><tr><td>
<img src="phasePlot.png" alt="" width="850px" />&nbsp;</td>
<td></td></tr></table>
<h2>Matlab code </h2>
<p><a href="../codes/TMac.zip">Download the code</a></p>
<h2>Citation</h2>
<p><i>Y. Xu, R. Hao, W. Yin, and Z. Su</i>. Parallel matrix factorization for low-rank tensor completion, Inverse Problems and Imaging, 9(2015), 601&ndash;624.</p>
<h2>References</h2>
<p><img class="eq" src="eqs/8209412804330245758-130.png" alt="[1]" style="vertical-align: -5px" />  S. Gandy, B. Recht, and I. Yamada, Tensor completion and low-<img class="eq" src="eqs/14080042351-130.png" alt="n" style="vertical-align: -0px" />-rank tensor recovery via convex optimization, Inverse Problems, 27(2011), p. 025010.</p>
<p><img class="eq" src="eqs/8209412804333245623-130.png" alt="[2]" style="vertical-align: -5px" /> J. Liu, P. Musialski, P. Wonka, and Jieping Ye, Tensor completion for estimating missing values in visual data, IEEE Transactions on Pattern Analysis and Machine Intelligence, (2013), pp. 208-220.</p>
<p><img class="eq" src="eqs/8209412804332245752-130.png" alt="[3]" style="vertical-align: -5px" />  Z. Wen, W. Yin, and Y. Zhang, Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm, Mathematical Programming Computation, (2012), pp. 1-29</p>
</div>
<div id="footer">
<div id="footer-text">
generated 2019-09-22, by <a href="http://jemdoc.jaboc.net/">jemdoc</a> (modified)
</div>
</div>
</div> <!-- Yangyang: "wrapper" class for new style -->
</body>
</html>
